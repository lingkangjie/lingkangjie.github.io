<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>笔记集</title>
    <description></description>
    <link>http://localhost:4000/huxblog-boilerplate/</link>
    <atom:link href="http://localhost:4000/huxblog-boilerplate/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 06 Jun 2019 18:22:06 +0800</pubDate>
    <lastBuildDate>Thu, 06 Jun 2019 18:22:06 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Linux Command Line</title>
        <description>&lt;h1 id=&quot;basic-commands&quot;&gt;basic commands&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;$ date, cal(calender), df (current amount of free space on disk driver), free (free memory)&lt;/li&gt;
  &lt;li&gt;ls, -l use a long listing format, -t sort the result by file’s modification time, –reverse -r reverse the results, -S sort results by file size, -h human readable.&lt;/li&gt;
  &lt;li&gt;$ file, determining the file’s type. $ less, viewing the file contents with less.&lt;/li&gt;
  &lt;li&gt;$ cp, -i –interactive prompt before overwrite, -r –recursive, -u –update copy only when the SOURCE file is newer than the distination file or when the destination file sis missing. -v –verbose, display copy information.&lt;/li&gt;
  &lt;li&gt;$mv file1 file2 dir1 move file1 and file2 to dir1.&lt;/li&gt;
  &lt;li&gt;$rm -i –interactive, -v –verbose, usage just like above.&lt;/li&gt;
  &lt;li&gt;change stander output using &amp;gt;, such as $ls -l &amp;gt; tmp.txt. Using » to append new output, such as $ls -l /usr/bin » tmp.txt.&lt;/li&gt;
  &lt;li&gt;redirecting standard error to a file:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ls -l /bin/usr 2&amp;gt; ls-error.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;When &lt;code class=&quot;highlighter-rouge&quot;&gt;$ less ls-error.txt&lt;/code&gt;, ouput &lt;code class=&quot;highlighter-rouge&quot;&gt;ls: cannot access '/bin/usr': No such file or directory&lt;/code&gt;.
Here file discriptor, 1, 2, 3 is represented standard input, standard ouput, standard error, respectively. The descriptor 2 is placed immediately befor the redirection operator &amp;gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;redirecting standard input and standard error to one file. Append method: &lt;code class=&quot;highlighter-rouge&quot;&gt;$ls -l /bin/usr &amp;gt;&amp;gt; ls-error.txt 2&amp;gt;&amp;amp;1&lt;/code&gt;. First, we redirect standard output to file ls-error.txt and then we redirect file descriptor 2 (standard error) to file descriptor 1 (standard output) using the notation 2&amp;gt;&amp;amp;1. Notice that the order of the redirections is significant. The redirection of standard error must occur &lt;em&gt;after&lt;/em&gt; redirecting output or it does not work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&amp;gt;&lt;/code&gt; to redirect both standard output and standard error to file. &lt;code class=&quot;highlighter-rouge&quot;&gt;$ls -l /bin/usr &amp;amp;&amp;gt;&amp;gt; ls-error.txt&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$cat, changing the standard input. Display file: $cat filename. Concatenate files together: $cat file1 file2 &amp;gt; merged-files. Read inputs from standard Input IO to files: $cat &amp;gt; tmp.txt (use ctrl + d to tell &lt;em&gt;cat&lt;/em&gt; that it has reached end of file (EOF).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Pipelines. The difference between &amp;gt; and&lt;/td&gt;
          &lt;td&gt;: one connects a command with file with file, other connects a command with a command, command1 &amp;gt; file1, command1&lt;/td&gt;
          &lt;td&gt;command2.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ls /bin /usr/bin | sort | uniq -d | less&lt;/code&gt;, use uniq to remove any duplicates from the output of sort command, use -d to see the list of duplicates instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;wc, word count, used to display the number of lines, words, and bytes contained in files, -l limits its output to only report lines.&lt;code class=&quot;highlighter-rouge&quot;&gt;$ls /bin /usr/bin | sort | uniq | wc -l&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;grep, print lines matching a pattern. grep pattern [file…].&lt;/li&gt;
  &lt;li&gt;tail, head, print end/first part of files, such as head -n 10, prints first 10 lines.&lt;/li&gt;
  &lt;li&gt;tee, read from standard input and write to standard output and files, it is useful for capturing a pipeline’ contents at an intermediate stage of processing, &lt;code class=&quot;highlighter-rouge&quot;&gt;$ls /usr/bin | tee ls.txt | grep zip&lt;/code&gt;, capture the standard output of ls to ls.txt before grep filters the pipeline’s contents.&lt;/li&gt;
  &lt;li&gt;Brace expansion:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lkj@lkj:~/tmp/Notes$ echo N_{001..11}
N_001 N_002 N_003 N_004 N_005 N_006 N_007 N_008 N_009 N_010 N_011
lkj@lkj:~/tmp/Notes$ echo {Z..A}
Z Y X W V U T S R Q P O N M L K J I H G F E D C B A
lkj@lkj:~/tmp/Notes$ echo a{A{1,2},B{3,4}}b
aA1b aA2b aB3b aB4b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Command Substitution, allows us to use the output of a command as an expansion. &lt;code class=&quot;highlighter-rouge&quot;&gt;$ file $(ls -d /usr/bin/* | grep zip)&lt;/code&gt;, means after &lt;em&gt;grep&lt;/em&gt; filters, &lt;em&gt;file&lt;/em&gt; the results.&lt;/li&gt;
  &lt;li&gt;Move cursor around, as I use tmux and xshell at the same time, I find that &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl +a&lt;/code&gt;(move cursor to the beginning of the line), &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+e&lt;/code&gt; (move the cursor to end end of the line), &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + k&lt;/code&gt;(kill text from the cursor location to the end of line), &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+u&lt;/code&gt;(kill text from the cursor location to the beginning of the line), &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+f&lt;/code&gt; (the same as right arrow key), &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+b&lt;/code&gt; (conflicts with tmux ctrl+b, but still works alternately)  work.&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;History. Search history: $history&lt;/td&gt;
          &lt;td&gt;grep /usr/bin. When we get the line number of the command in the history list, we can execute it by &lt;code class=&quot;highlighter-rouge&quot;&gt;!number&lt;/code&gt;, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;!80&lt;/code&gt;. Search history in reverse: first &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+r&lt;/code&gt; to enter the reverse search env, then type the contents you want to search, such &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin&lt;/code&gt;, use &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+j&lt;/code&gt; to copy the command to our current command line for futher editing or execute. &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+p&lt;/code&gt;, move to previous history entry. &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl+n&lt;/code&gt;, move the next history entry. &lt;code class=&quot;highlighter-rouge&quot;&gt;!!&lt;/code&gt; repeat the last command.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;file property.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;drwxr-xr-x 12 lkj lkj    4096 May 26 18:21 ..
-rw-rw-r--  1 lkj lkj       0 May 26 19:15 foo.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;- : a regular file, d : directory, l : symbolic link, and the remaining file attributes are always “rwxrwxrwx”, c : character file, b : block special file.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“rwx(Owner) rwx(group) rwx(world, everybody else)”, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt; allows file to be opened and read, &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; allows to written to or truncated, &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; allows to be treated as a program and executed.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;chmod, change file mode, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;$chmod 600 foo.txt&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;$chmod 777 foo.txt&lt;/code&gt;. Another way, u, g, o, a represent short for user, short for owner, short for others or world or everybody else, short for all. +, - represent add, minus attributes. r, w, x represent attributes. e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;u+x&lt;/code&gt; add execute permission for the owner, &lt;code class=&quot;highlighter-rouge&quot;&gt;u-x&lt;/code&gt; remove execute permission from the owner, &lt;code class=&quot;highlighter-rouge&quot;&gt;+x&lt;/code&gt; as the same as &lt;code class=&quot;highlighter-rouge&quot;&gt;a+x&lt;/code&gt; add execute permission for owner, group, world.&lt;/li&gt;
  &lt;li&gt;chown, change file owner and grop. chgrp, change group ownership. passwd [user], change your password.&lt;/li&gt;
  &lt;li&gt;By default, &lt;code class=&quot;highlighter-rouge&quot;&gt;ps&lt;/code&gt; only show the processes associated with the current terminal seesion, tty(short for teletype, refers to controlling terminal). &lt;code class=&quot;highlighter-rouge&quot;&gt;ps x&lt;/code&gt; will show all processes.&lt;/li&gt;
  &lt;li&gt;top
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;top - 19:47:36 up  1:27,  4 users,  load average: 0.07, 0.09, 0.09
Tasks: 317 total,   1 running, 226 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.5 us,  0.8 sy,  0.0 ni, 98.1 id,  0.5 wa,  0.0 hi,  0.0 si,  0.0 st
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;top&lt;/code&gt; the name of the program. &lt;code class=&quot;highlighter-rouge&quot;&gt;up 1:27&lt;/code&gt; the machine was running 1 hour and 27 minutes. &lt;code class=&quot;highlighter-rouge&quot;&gt;load avergae&lt;/code&gt;, the number of processes that are in a runnable state and are sharing the CPU, the first is average for last 60 seconds, next is previous 5 minutes, next is previous 15 minutes, values less than 1.0 indicate that the machine is not busy. &lt;code class=&quot;highlighter-rouge&quot;&gt;0.5 us&lt;/code&gt; 0.5% of CPU is being used for &lt;strong&gt;user processes&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;0.8 sy&lt;/code&gt; 0.8% of CPU is being used for &lt;strong&gt;system(kernel)&lt;/strong&gt; processes, &lt;code class=&quot;highlighter-rouge&quot;&gt;0.0 ni&lt;/code&gt; 0.0% of CPU is being used by “nice”(low-priority) processes. &lt;code class=&quot;highlighter-rouge&quot;&gt;98.3 id&lt;/code&gt; 98.3% of CPU is idle. &lt;code class=&quot;highlighter-rouge&quot;&gt;0.5 wa&lt;/code&gt; 0.5% of CPU is waiting for I/O.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The most importance startup file &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt; for Non-Login Shell Session, it is always read.&lt;/li&gt;
  &lt;li&gt;export, set export attribute for shell variables, that is to say update $Variable. e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;$ PATH=$PATH:$HOME/bin&lt;/code&gt;, and then update $PATH by &lt;code class=&quot;highlighter-rouge&quot;&gt;export PATH&lt;/code&gt;.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lkj@lkj:~/tmp/Notes$ foo=&quot;ok&quot;
lkj@lkj:~/tmp/Notes$ foo=$foo:$HOME/data
lkj@lkj:~/tmp/Notes$ echo $foo
ok:/home/lkj/data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Which files should we modify for establishing environment? To add directories to your &lt;em&gt;PATH&lt;/em&gt; of define additional environment variables, place those changes in &lt;strong&gt;.bash_profile&lt;/strong&gt;( or &lt;strong&gt;.profile&lt;/strong&gt; in Ubuntu). For everything else, place the changes in &lt;strong&gt;.bashrc&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;In Nano editor, ^ means Ctrl, such as, ^X means Ctrl-x.&lt;/li&gt;
  &lt;li&gt;cat -A filename, -A show all. -n show number lines, -s suppress the output of multiple blank lines.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ls -l /usr/bin | sort -nrk 5 | head&lt;/code&gt;, -n preform sorting based on the numeric evaluation of a sting, using this option allows sorting to be performed on numeric values rather than alphabetic values. -r –reverse. -k –key=field1[,field2], sort based on a key field located from field1 to field2 rather than the ectire line.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lkj@lkj:~/tmp/Notes$ ls -l /usr/bin | sort -nrk 5 | head
-rwxr-xr-x 1 root root    79780856 Jan 24 18:49 dockerd
-rwxr-xr-x 1 root root    38199680 Jan 24 18:49 docker
-rwxr-xr-x 1 root root    38158448 Jan 24 18:49 docker-containerd
-rwxr-xr-x 1 root root    20919664 Jan 24 18:49 docker-containerd-ctr
-rwxr-xr-x 1 root root    14924104 Mar 12  2018 doxygen
-rwxr-xr-x 1 root root    12849608 Jan 30 01:50 snap
-rwxr-xr-x 1 root root    11661624 Jan 24 18:49 docker-runc
-rwxr-xr-x 1 root root     9074600 Jan 30 01:50 snapctl
-rwxr-xr-x 1 root root     7602672 Apr  9  2018 gdb
-rwxr-xr-x 1 root root     6444464 Apr 24  2018 ctest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;wildcard-and-regular-expressions&quot;&gt;wildcard and regular expressions&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;* matches any characters&lt;/li&gt;
  &lt;li&gt;? matches any single character&lt;/li&gt;
  &lt;li&gt;[characters] matches any character that is member of the set &lt;strong&gt;characters&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;[!characters]&lt;/li&gt;
  &lt;li&gt;[[:class:]] matches any character that is a member of specifiled class.&lt;/li&gt;
  &lt;li&gt;[:alnum:] matches any alphanumeric character, equivalent to [A-Za-z0-9]. [:word:], the same as [:alnum:], with the addition of underscore(_) character. [:alpha:] matches any alphabetic character. [:digit:] matches any numeral. [:lower:] lowercase. [:upper:] uppercase. [:blank:], includes the space and tab characters.&lt;/li&gt;
  &lt;li&gt;[[:upper:]]* any file beginning with an uppercase letter.&lt;/li&gt;
  &lt;li&gt;*[[:lower:]123] any file ending with lowercase letter or the numerals 1 2 3.&lt;/li&gt;
  &lt;li&gt;$grep [options] regex [file…]. &lt;code class=&quot;highlighter-rouge&quot;&gt;options&lt;/code&gt;, -i –ignore-case, ignore uppercase and lowercase characters. -v –invert-match, -c –count print the number of matches instead of the lines themselves. -l –file-with-matches print the name of each file that contains a match instead of the lines themselves. -L –file-without-match. -n –line-number, prefix each matching line with the number of the line within the file. -h –no-filename, for multi-file searchs, suppress the output of filenames.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ grep -h '.zip' dirlist-*.txt (any character)
$ grep -h '^zip' dirlist-*.txt (beginning of the line)
$ grep -h 'zip$' dirlist-*.txt (end of the line)
$ grep -n '^$' dirlist-*.txt (will match blank lines)
$ grep -h '^[A-Za-z0-9]' dirlist-*.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 03 Jun 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/huxblog-boilerplate/2019/06/03/Linux-Command-Line/</link>
        <guid isPermaLink="true">http://localhost:4000/huxblog-boilerplate/2019/06/03/Linux-Command-Line/</guid>
        
        <category>Linux</category>
        
        
      </item>
    
      <item>
        <title>Rethinking Model Scaling for Convolutional Neural Networks</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Authors: Mingxing Tan, Quoc V.Le. Notes by Lingkangjie, 2019-06-04&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;The authors use neural architecture search  method to design a new baseline network and scale it up to obtain family of models, called EfficientNets. This kind of ConvNets’ design method is more automatic and accuracy in the feature. What is model scaling meaning? Model scaling is something that develops a fixed resource budget, basic module, and then scales them up for better accuracy, like ResNet-101 et al.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In figure 1, the author show the relationship between number of parameters and Imagenet top 1 accuracy. Obviously, the less number of parameters of a network and higher top 1 accuracy indicates that the network is more efficient. Compared with the state-of-the-art network, EfficientNets achieves 8.4x smaller and 6.1x faster than GPipe on inference.&lt;/li&gt;
  &lt;li&gt;the way to scale up ConvNets: by depth, or width, or image resolution, corresponding to image depth, width and image size.&lt;/li&gt;
  &lt;li&gt;Traditionally, we scale the ConvNets only increase one dimension of network width, depth, or resolution. The authors propose a method called &lt;strong&gt;compound scaling method&lt;/strong&gt; that uniformly scales all three dimensions with a fixed ratio. Specifically, the authors empirically show that if we want to use 2&lt;sup&gt;N&lt;/sup&gt; times more computaional resources, then we can simply increase the network depth by alpha&lt;sup&gt;N&lt;/sup&gt;, width by beta&lt;sup&gt;N&lt;/sup&gt;, and image size by gamma&lt;sup&gt;N&lt;/sup&gt;, where alpha, beta and gamma are constant coefficients determined by a small grid search on the original small model.&lt;/li&gt;
  &lt;li&gt;one purpose of this work is that, since architecture search becomes increasingly popular in designing ConvNets, how to apply these techniques for larger models? The authors say, okay, we design a way to scale the baseline network to a super large ConvNets.&lt;/li&gt;
  &lt;li&gt;Why we need to scale the depth, width and resolution of the network at the same time? Intuitively, if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compound-model-scaling&quot;&gt;Compound Model Scaling&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The purpose of this paper is that trying to expand the network length, width, and/or resolution without changing pre-defined Conv operator in the baseline network. In order to reduce the design space, the authors restrict that all layers must be scaled uniformly with constant ratio.&lt;/li&gt;
  &lt;li&gt;Common way used by many ConvNets to design a larger network is scaling network depth. However, deeper networks are more difficult to train due to the vanishing gradient problem. Some techniques used to alleviate these problem, such as skip connections, batch normalization et al. Scaling network width is commonly used for small size models, as wider networks tend to be able to capture more fine-grained features and are easier to train. However, extremely wide but shallow networks tend to have difficulties in capturing higher level features. Higher resolution such as from 224x224 to 480x480, and to 600x600 also useful for improving accuracy. However, only scale the depth, or width, or resolution of the ConvNets, the accuracy gain quickly saturate after reaching 80%, demonstrating the limitation of single dimension scaling.&lt;/li&gt;
  &lt;li&gt;The authors observe that the resolution, width and depth of ConvNets are not isolated.It is critical to balance all dimensions of network width, depth and resolution during ConvNet scaling.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;efficientnet-architecture&quot;&gt;EfficientNet Architecture&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Since model scaling does not change layer operators in baseline network, having a good baseline network is also critical. So the authors design a new mobile-size baseline, called EfficientNet.&lt;/li&gt;
  &lt;li&gt;The paper search to optimize FLOPS rather than latency since it is not targeting any specific hardware device.&lt;/li&gt;
  &lt;li&gt;The main building block for the baseline network is &lt;a href=&quot;https://arxiv.org/pdf/1801.04381.pdf&quot;&gt;mobile inverted bottleneck&lt;/a&gt; MBConv coming from MobileNet. Table 1 tells us the architecture of baseline network called EfficientNet-B0.&lt;/li&gt;
  &lt;li&gt;How to scale EfficientNet? The authors define the scale factor of depth, width, and resolution for  ConvNet are alpha&lt;sup&gt;N&lt;/sup&gt;, beta&lt;sup&gt;N&lt;/sup&gt;, gamma&lt;sup&gt;N&lt;/sup&gt; respectively. And alpha, beta, gamma are constrained to that &lt;code class=&quot;highlighter-rouge&quot;&gt;alpha * beta * beta * gamma * gamma&lt;/code&gt; approximates to 2, and alpha, beta, gamma are must bigger than 1 or equal to 1.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;compound scaling method&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;step 1 is fix &lt;code class=&quot;highlighter-rouge&quot;&gt;N=1&lt;/code&gt;, and use grid search to get the optimized alpha, beta, gamma. In baseline network EfficientNet-B0, the best values are alpha=1.2, beta=1.1, gamma=1.15.&lt;/li&gt;
      &lt;li&gt;Step 2 is fix alpha, beta, gamma, scale up baseline network with different &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the paper, authors use EfficientNet-B0 with alpha=1.2, beta=1.1, gamma=1.15 as the best case of baseline and scale the network up with different &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;. Notice the final depth, width and resolution size for a network are alpha&lt;sup&gt;N&lt;/sup&gt;, beta&lt;sup&gt;N&lt;/sup&gt;, gamma&lt;sup&gt;N&lt;/sup&gt; respectively.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;At last, the EfficientNet achieve new state-of-the-art accuracy for 5 out of 8 datasets, with 9.6x fewer parameters on average. I notice that the results coming from EfficientNet are very approximate to other state-of-the-art models, meaning that of course EfficientNet is the best with less FLOPS also.&lt;/li&gt;
  &lt;li&gt;The paper also compares the scale only one dimension of network (by depth, by width, or by resolution) and its &lt;strong&gt;compound scale&lt;/strong&gt; method, the result shows that its method can further improve accuracy. The model with compound scaling tends to focus on more relevant regions with more object details, while other models are either lack of object details or unable capture all objects in the images.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The proposed scaling method is simple but highly effective, which enables us to easily scale up a baseline ConvNet to any target resource constrains in a more principled way, whlie maintaining model efficiency.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 01 Jun 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/huxblog-boilerplate/2019/06/01/EfficientNet/</link>
        <guid isPermaLink="true">http://localhost:4000/huxblog-boilerplate/2019/06/01/EfficientNet/</guid>
        
        <category>Deep Learning</category>
        
        <category>Machine Learning</category>
        
        <category>Neural Networks</category>
        
        
      </item>
    
      <item>
        <title>Notes for A Survey of Object detection</title>
        <description>&lt;h2 id=&quot;object-detection-in-20-years-a-survey&quot;&gt;Object Detection in 20 years: A Survey&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;authors: Zhengxia Zou, Zhenwei Shi, Yuhong Guo, et al.. Notes by Lingkangjie, 2019-06-05&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Introduce the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and state-of-the-art detection methods, and applications, such as pedestrian detection, face detection, text detection, etc.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Object detection techniques form the basis of many other computer vision task, such as instance segmentation, image captioning, object tracking, etc. Object detection has now been widely used in many real-world applications, such as autonomous driving, robot vision, video, surveillance, etc.&lt;/li&gt;
  &lt;li&gt;Milestone detectors: VJ detector (2001), HOG detector (2005), DPM + Bounding box regression (2008,2010), two-stage detector {RCNN (2014), SPPNet (2014), Fast RCNN (2015), Pyramid Networks (2017)}, one-state detector {YOLO (2016,2017), SSD (2016), Retina-Net (2017)}&lt;/li&gt;
  &lt;li&gt;What are the difficulties and challenges in object detection? Some Common challenges includes objects under different viewpoints, illuminations, intraclass variations, object rotation, scale changes(e.g., small objects), accurate object localization, dense and occluded object detection, speed up of detection, etc. As different detection tasks have totally different objectives and constraints, their difficulties vary from each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;object-detection-in-20-years&quot;&gt;Object detection in 20 years&lt;/h3&gt;

&lt;h4 id=&quot;detectors&quot;&gt;Detectors&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;VJ detector introduced three important techniques: integral image, feature selection, detection cascades.&lt;/li&gt;
  &lt;li&gt;HOG detector introduces multiple times image scaling technique while keeping the size of detection window unchanged. HOG detector was motivated primarily by the problem of pedestrian detection.&lt;/li&gt;
  &lt;li&gt;Deformable Part-based Model (DPM), is the winners of VOC-07, -08, and -09 detection challenges. The key idea behind DPM is “divide and conquer”, where the training can be simply considered as the learning of a proper way of decomposing an object, and the inference can be considered as an ensemble of detections on different object parts. A typical DPM detector consists of a root-filter and a number of part-filters. Later, some new ideas merged into DPM such as mixture models, hard negative mining, bounding box regression, etc.&lt;/li&gt;
  &lt;li&gt;The key idea behind RCNN is simple: get object proposals by selective search, and each proposal is rescaled to a fixed size image and fed into a CNN model trained on ImageNet to extract features, finally use linear SVM classifiers to classify. There are total 2000 boxes from one image.&lt;/li&gt;
  &lt;li&gt;SPPNet introduces a Spatial Pyramid Pooling (SPP) layer, which enable a CNN to generate a fixed-length representation regardless of the size of image/region of interest without rescaling it.&lt;/li&gt;
  &lt;li&gt;Inspired by R-CNN and SPPNet, a new network called Fast RCNN which enables us to simultaneously train a detector and a bounding box regressor under the same network configurations was proposed. Fast RCNN still uses proposal detection technique.&lt;/li&gt;
  &lt;li&gt;“Can we generate object proposals with a CNN model”, So Faster RCNN came into being. The main contribution of Faster-RCNN is the introduction of Region Proposal Network (RPN) that enables nearly cost-free region proposals.&lt;/li&gt;
  &lt;li&gt;Most individual blocks of an object detection system, e.g., proposal detection, feature extraction, bounding box regression, etc, have been gradually integrated into a unified, end-to-end learning framework called one-stage detection.&lt;/li&gt;
  &lt;li&gt;You Only Look Once (YOLO), divides the image into regions and predicts bounding boxes and probabilities for each region simultaneously. YOLO suffers from a drop of the localization accuracy compared with two-stage detectors, especially for some small objects. As YOLO use pre-defined anchors to locate an object.&lt;/li&gt;
  &lt;li&gt;Single Shot MultiBox Detector (SSD). The main contribution of SSD is the introduction of the multi-reference and multi-resolution detection techniques. Any previous detectors detect objects of different scales on different layers of the network (e.g, YOLO has three heads to detect three kinds of scaling), and SSD only runs detection on their top layers.&lt;/li&gt;
  &lt;li&gt;RetinaNet introduces a new loss function suited for one-stage detection called “focal loss”. Focal loss enables the one-stage detectors to achieve comparable accuracy of two-stage detectors while maintaining very high detection speed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;datasets&quot;&gt;Datasets&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Pascal VOC (PASCAL Visual Object Classes), tasks including image classification, object detection, semantic segmentation and action detection.&lt;/li&gt;
  &lt;li&gt;ILSVRC (ImageNet Large Scale Visual Recognition Challenge)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;VOC07: 5k training images, 12k annotated objects, 20 classes.
VOC12: 11k training images, 27k annotated objects, 20 classes.
ILSVRC: 517k images, 534k annotated objects, 200 classes.
MS-COCO-17: 164k images, 897k annotated objects, 80 classes.
OID (Open Images Detection): 1910k images, 15440k annotated objects, 600 categories.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Object detection tasks: pedestrian detection, remote sensing target detection, face detection, text detection, traffic sign/light detection, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;metrics&quot;&gt;Metrics&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;In the early time, use “miss rate vs. false positives per-window (FPPW)”. From VOC2007, introduces Average Precision (AP) and mean AP (mAP). MS-COCO AP is averaged over multiple IoU thresholds between 0.5 (coarse localization) and 0.95 (perfect localization).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;technical-evolution-in-object-detection&quot;&gt;Technical Evolution in Object Detection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Early time, Recognition-by-components is the core idea of image recognition and object detection. The researchers propose rule-based or template based approaches by comparing the similarity between the object components, shapes and contours. In 1998-2005, people use wavelet feature representations to do object detection by transforming an image from pixels to a set of wavelet coefficients. Later (2005-2012), gradient-based representations of statistical model have been popular.&lt;/li&gt;
  &lt;li&gt;The history of using CNN to detecting objects can be traced back to 1990s. In the early day, some Y. LeCun et al. works can be considered as the prototype of today’s fuuly convolutional networks (FCN).&lt;/li&gt;
  &lt;li&gt;Technical evolution of multi-scale detection:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;T1: feature pyramids and silding windows (VJ Det., HOG, DPM, etc.)
T2: detection with object proposals (RCNN, SPPNet, Fast RCNN, Faster RCNN)
T3: deep regression (YOLO)
T4: multi-reference detection (SSD, YOLOv2)
T5: multi-resolution detection (FPN, RetinaNet)

years: --2001-...-2010--2011--2012--2013---2014---2015---2016---2017---2018---2019
          ------------ T1 ------------------&amp;gt;|
                   |&amp;lt;------------- T2-------------&amp;gt;|
                                     |&amp;lt;--------- T3 -----&amp;gt;|
                                                   |&amp;lt;------- T4--------
                                                          |&amp;lt;------ T5----------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;To answer the question “ is there a unified multi-scale approach to detect objects of different aspect ratios?”, then researchers introduce “object proposals” (T2).&lt;/li&gt;
  &lt;li&gt;Pros of T3: simple, easy. Cons of T3: the localization may not be accurate enough especially for some small objects (T4 has latter solved this problem).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-reference/Multi-resolution detection. The main idea of T4 is to pre-define a set of reference boxes (anchor boxes) with different sizes and aspect-ratios at different locations of an image, and then predict the detection box based on these reference. T5 is a technique to detect objects of different scales at different layers of the network. T5 Detects larger objects in deeper layers and smaller ones in shallower layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Technical evolution of bounding box regression. BB aims to refine the location of a predicted bounding box based on the initial proposal or the anchor box.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;T1: without bounding box regression(VJ Det., HOG, etc.)
T2: from bounding box to bounding box (DPM)
T3: from feature to bounding box (SPPNet, Fast RCNN, 
    Faster RCNN, YOLO, SSD, FPN, RetinaNet, TridentNet, etc.)

years: --2001-........--2008---------------2013---2016---2017---2018---2019
          ---- T1 -------&amp;gt;|
                          |&amp;lt;----- T2--------&amp;gt;|
                                                   |&amp;lt;--------- T3 -------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Technical evolution of context priming. Visual objects are usually embedded in a typical context with the surrounding environments. We use the associations among objects and environments to perform object detection. This kind of method called context priming.
    &lt;ul&gt;
      &lt;li&gt;Detection with local context. Some researchers found that by incorporating a small amount of background information improves the accuracy of pedestrian detection early in 2005.&lt;/li&gt;
      &lt;li&gt;Detection with global context. The first way is to take advantage of large receptive field (even larger than the input image) or global pooling operation of a CNN feature. The second way is to think of the global context as a kind of sequential information such as in video, and learn it with recurrent neural networks.&lt;/li&gt;
      &lt;li&gt;Context interactive. Context interactive refers to the piece of information that conveys by the interactions of visual elements, such as the constrains and dependencies.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Technical evolution of non-maximum suppression. NMS has been gradually developed into three groups of methods:
    &lt;ul&gt;
      &lt;li&gt;traditional greedy selection ( DPM, RCNN, Fast RCNN, Faster RCNN, YOLO, SSD, FPN, RetinaNet etc.): for a set of overlapped detections, the bounding box with the maximum detection score is selected while its neighoring boxes are removed according to a predefined overlap threshold (say, 0.5). Cons of this method: the top-scoring box may not be the best fit, it may suppress nearby objects, it does not suppress false positives.&lt;/li&gt;
      &lt;li&gt;bounding box aggregation: takes full consideration of object relationships and their spatial layout.&lt;/li&gt;
      &lt;li&gt;learning to NMS: think of NMS as a filter to re-score all raw detections and to train the NMS as part of a network in an end-to-end fashion.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;./imgs/NMS.jpg&quot; alt=&quot;evolution of non-max suppresion&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technical evolution of hard negative mining (HNM). In real scene of object detection, the imbalance between backgrounds and objects could be as extreme as 10&lt;sup&gt;4&lt;/sup&gt; ~ 10&lt;sup&gt;5&lt;/sup&gt; background windows to every object. HNM aims to deal with the problem of imbalanced data during training.
    &lt;ul&gt;
      &lt;li&gt;Bootstrap. Bootstrap in object detection refers to a group of training techniques in which the training starts with a small part of background samples and then iteratively add new misclassified backgrounds during the training process.&lt;/li&gt;
      &lt;li&gt;balance the weights between the positive and negative windows. However, this kind of method cannot completely solve the imbalanced data problem.&lt;/li&gt;
      &lt;li&gt;design new loss functions, put more focus on hard, misclassified examples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acceleration-of-object-detection&quot;&gt;Acceleration of object detection&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Level 1: speed up of numerical computations (integral image, FFT, vector quantization, reduced rank approx)&lt;/li&gt;
  &lt;li&gt;Level 2: speed up of detector engine (network pruning and quantification, lightweight network design)&lt;/li&gt;
  &lt;li&gt;Level 3: speed up of detection pipeline (feature map shared computation, speed up of classifier, cascaded detection)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;feature-map-shared-computation&quot;&gt;feature map shared computation&lt;/h4&gt;
&lt;p&gt;Compute the feature map of the whole image only once before sliding window on it, such as how HOG feature map comes. Another example coming from CNN, weight sharing. Scale computational redundancy, directly scale the features rather than the images.&lt;/p&gt;

&lt;h4 id=&quot;speed-up-of-classifiers&quot;&gt;speed up of classifiers&lt;/h4&gt;
&lt;p&gt;Linear classifiers are low computational complexity, while kernel-based SVM has higher accuracy. Use reduced Set Vectors, piece-wise linear approximation for decision boundary, sparse encoding to speed up traditional SVM.&lt;/p&gt;

&lt;h4 id=&quot;cascaded-detection&quot;&gt;cascaded detection&lt;/h4&gt;
</description>
        <pubDate>Tue, 28 May 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/huxblog-boilerplate/2019/05/28/survey-of-OD/</link>
        <guid isPermaLink="true">http://localhost:4000/huxblog-boilerplate/2019/05/28/survey-of-OD/</guid>
        
        <category>Deep Learning</category>
        
        <category>Machine Learning</category>
        
        <category>Object detection</category>
        
        
      </item>
    
      <item>
        <title>哲学</title>
        <description>&lt;h2 id=&quot;金刚经论文集注&quot;&gt;金刚经论文集注&lt;/h2&gt;

&lt;p&gt;金刚经中常见的主题：修行、空、经、心、生命、时间、空间。何为“虚空”？“虚而无形质，空而无障碍，故名虚空”，虚空具有周遍、不动、无尽、永恒的属性，包括上下、东南西北（四方）、东南、西南、东北、西北（四维）共10个方向。虚空并非空无一物，而是包含着无数的“微尘”和“世界”。“三千大千世界所有微尘”，“以三千大千世界碎为微尘”，虚空中包含无数的微尘，微尘组成世界，世界又可碎为微尘。世界，世指时间，界指空间，世界又可称为世间，“世”有流变之义，“间”有间隔之义。时间流逝有两个版本，一个是时间的流逝是时间从未来向自我的移动，称为“时间移动”，另一个版本是时间的流逝是自我从过去向未来的移动，称为“自我的移动”。心经中常见“时间移动”。“初日分以恒河沙等身布施…”，这里可以体现了“时间的持续是空间的长度”，一个世界从成到消亡，经历了成、住、坏、空四个阶段组成一个劫。时间是无止境的循环。时间的流逝是时间从未来朝向我的移动。生命与时间具有恒等性，先（前）世-今世-后世反映了时间的流动，继承了时间的循坏性。生命以某种物质形式来承载每个生命，不同的世可能具有不同的生命形式，正如三恶道、三善道，六道轮回。六道：天、人、阿修罗、畜生、恶鬼、地狱。“堕额道”说明了有方向，暗示着善道居高位、恶道居低处，与好的是上、坏的是下基本方位隐喻一致。&lt;/p&gt;

&lt;p&gt;“无住”思想阐述。佛法就是要解决心的问题，金刚经以无住为宗，正显般若智用之相貌。“无住”并非真无住，“无住者，非无所住也，乃不着于住也”，不是无所住，而是当住于佛法真理，佛身不执于物，心不滞于物。“所谓佛法者”，“如来所说三十二相”，佛法本难以言传，佛祖为了拯救众生而说不可说之法，如此无上妙法，难以用文字表达，倘若执滞于文字言传，终不能登堂入室，见识大雅，故用此话来告诫众生，不要拘泥于文字，也是“无住”一种思想表现。无住，非无住也，无住错误的真理，当身趋堂室，窥见大宝，随后因住也，而不当住于文字言传。观察佛祖金刚经中的日常行为，“尔时世尊食时，…，敷座而坐”，日常的衣食住行表现了性空即缘起，缘起即性空的中道，不有附住之心，人生苦的根源来自于世俗生活，“其实微尘世界，皆无自性，非有实体，众生执有，抟取色身，假立一合相，但不过影像而已”，不住声香味触发，行为亦无住也。&lt;/p&gt;

&lt;h2 id=&quot;厚黑学&quot;&gt;《厚黑学》&lt;/h2&gt;
&lt;h3 id=&quot;缘起&quot;&gt;缘起&lt;/h3&gt;
&lt;p&gt;人生而自由，而无时无刻不在枷锁中。人受到环境、教育的潜移默化，行为受到束缚。人生即是竞争和生存，竞争就必有谋略，厚黑学是谋略中的一种。该书中以下8个方面进行说明如何厚黑：人性弱点、进退有方、做事为人、小人之智、立身行世、待人处世、求人办事、成败铁规。&lt;/p&gt;

&lt;h3 id=&quot;分体内容&quot;&gt;分体内容&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;厚黑，是从另一个角度去思考人性与方法。文人说话文绉绉，实属不适合理科生思维。咬文嚼字，这也行，那也行，方案如此众多，思考角度之繁杂，实属不适合理科生思维。尽管如此，我还是能够提炼出一些观点和notes。&lt;/li&gt;
  &lt;li&gt;我持有怀疑和吸收态度阅读这本《厚黑学》，对其中一些观点批判性吸纳，总和过去阅读积累，将这本书变薄，提纲挈领，各种林林总总，化为结构性思维。书中上至三皇五帝，下至民国初期，时间跨度之大、人物之繁多，背后其故事缘由，常人难以一窥，更难进行深度分析。孔孟学说、王阳明心学、刘曹谋略，滔滔如也，文字背后，缺乏一般性原理分析，更多是集中在某个特定时空事件下管中一窥。作者认为人性从上古到今天，经历了，素而实，华而不实，华而又实三个阶段，如今正处在第二阶段。作者把过往圣人、能者的成功的原因归为厚黑，难免不牵强附会也。一家之言，百家争相注释，厚黑之至，正如宗吾作者。厚黑的三大境界：厚如城墙、黑如煤炭，厚而硬、黑而亮，厚而无形、黑而无色。古今圣贤，能人志士，苟有事成，厚黑或是一个原因。&lt;/li&gt;
  &lt;li&gt;求官，求人办事妙法：空、贡、冲、捧、恐、送。空指空间上，空闲自己手头其他事，一心一意求。时间上，一次不行，第二次接着，今日不成，明日继续。贡指钻营，有孔必钻，无孔也制造孔，无孔也要钻。冲，吹牛，新闻舆论，在口头上能说会道，文字上善于营造新闻舆论氛围。捧，捧场，拍马屁。恐，恐吓之义，既然捧一个人，也给点他警醒，俗称把柄，捧+恐，一杨一抑，恐亦有度，亦有分寸。送，送东西，按照价值分大送，小送。做官，待人处事：空、恭、绷、凶、聋、弄。空指文字、批文玄妙，办事上看似雷厉风行，实际暗含退路，根本原则是绝不把自己牵连。恭，一恭上司，二恭上司亲戚朋友等与之相关的人物。绷，指仪表上赫然大人物，凛然不可侵犯。谁给权力，对谁就恭，反之则可绷，恭之不一定是上司，绷之也不一定是下属、百姓。凶，指行事看似有道德仁义，实际上只要达到自己目的，不必拘泥道德仁义也，但一定要装饰道德仁义。聋，有些事瞎子好，聋子好。有些文字诋骂，闭眼不看。弄，弄钱之义。&lt;/li&gt;
  &lt;li&gt;办事二法：锯箭法、补锅法。锯箭法，只做自己该做的，不以解决最终问题（医好人）为导向，相互推诿，只做自己的，我认为在程序化办事上，锯箭法是不可避免的，因为前者的结果，是后者的输入，唯一解决办法是顶层重新设计，把功能集中，减少低强度的operations的分散度。补锅法阐述了，为了突出自己的地位和不可替代性，把原始工作破坏，然后让自己再重新修补，有种权弄之义。我认为，这个问题的根本原因是，把太多功能的实现集中在一个客体上，恰好是锯箭法的反方向。锯箭法和补锅法，恰好是一对矛盾。上述思考是站在管理者角度上，站在个人角度上，通过锯箭法可以把责任推得一干二净，而又可以做完事，不受到谴责，通过补锅法，突出自己的价值，或者获取更多的价值（当前某些医院就有这个现象）。&lt;/li&gt;
  &lt;li&gt;所有的厚黑之法，务必糊上一层仁义道德，而且要有“名”，重视言正名顺，师出有名。&lt;/li&gt;
  &lt;li&gt;厚黑，也可以理解，厚指厚道，黑就是指代那种黑。一正一歪，一阳一阴，一正一奇，符合道家之万物相生相克之道，正奇相生，循环无端，厚中有黑，黑中有厚，为人做事确实如此也。&lt;/li&gt;
  &lt;li&gt;厚黑之法，千年未曾有系统性公布，却无时无刻被人们使用，或多或少。每个人都曾受到厚黑之法的影响。这是一种未曾被发掘的东西，却潜伏于文学、权谋、人与人沟通之中。宗吾通过他的认识，归纳总结厚黑之法，认为“这样”，“那样”的做法比较符合厚黑之道也。宗吾观二十四史，成败兴衰，潇潇洒洒几万字，上至文王勾践，夫子孟子，下达今外蛮夷列强，认为厚黑救国。我想，厚黑救不了国，科技才立国，民众尔虞我诈，上下不一心，国危矣！文人易误国，蛊惑视听，巧言令色，不可不防。古之治国，有两法，德治，法治，秦国焚书坑儒，就是想统一思想领域。秦国武治，强势之至，国强民富，至于暴戾乖张，则国蹦民卒。文武之治，方是正道。如今全球科技停滞不前，二十世纪两大重大发现，相对论和量子力学，带来的科技福利，逐渐消磨殆尽，国与国之间，为了争夺微薄应用性价值，大举贸易和军事大棒，厚黑救不了国，当前重要是真正重视科技创新，底层科学和技术的进步，独立自主，上下一心。历史上救国的，从不是歪门邪道，而是正远远大于邪。厚黑可防身，但毕竟只是一种系统性方法，不可仅为依赖。&lt;/li&gt;
  &lt;li&gt;心力论，在相对论中，光受到引力的影响，而弯曲，人心理也一样，受到心力影响，也产生颠倒之想。正所谓圆觉经中阐述，一切众生，无始以来，种种颠倒，妄认四大为自身相，六尘缘影为自心相。人性无本是善恶，是可以为善，可以为恶，王阳明言，性，心之体也，情，心之用也。&lt;/li&gt;
  &lt;li&gt;如何读书？以古为敌，读书，例如读古书，寻找他的缝隙，见缝插针，针锋相对，又假设他反驳，如此反复，愈攻愈深，如此读书，便可入理八分。以书为友，相互切磋，吸收书中优秀观点和理论方法。以书为徒，前人写的书，学识肤浅，总有提高地方，如阅读学生文字一样，批评其不对，知识是否在我之上？是也，认为遇到强敌，攻之。否也，批评之。&lt;/li&gt;
  &lt;li&gt;讲到厚黑学，根本离不开人性善恶论。人心最难理解，孟子认为人善，荀子认为人恶论，在我看，他们都在争论性之初，无法确定性之当前状态。乃至宗吾认为，人无善恶之分。我更倾向于与其争论之初，不如确定当前的状态为好，从MCMC和蒙特卡罗模拟方法看，只要抽样次数足够多，系统的最终状态和初始状态无关。我们除了研究人性，还要注意研究人的兽性。&lt;/li&gt;
  &lt;li&gt;宗吾所著的厚黑学，离不开孔孟之说，儒释道三家的思想，在整本书中，考之中外，恢诡谲怪，举莫能外，文字潇潇洒洒，对习惯快、准、直思维的理科人士而言，实属大块头。尽管如此，我把其中觉得有用的东西进行了提纲挈领，认为大体上不失一般性，至于艰涩灰暗的文字，历史典故，如何林林立立，唯交给文人了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;论存在与虚无&quot;&gt;论存在与虚无&lt;/h2&gt;
&lt;p&gt;论存在与表象。二元论观点把存在与表象放在一起，认为存在即有表象，对立起来，相信本体，实际上我们有一种方法可以破相。显现（就是表象的显露过程）本身没有表明存在，例如力这个“本体”可以通过各种现象来显现，但它只是这些效应的总体，一个系列的显现，它背后没有什么东西，只是表明力它自己和整个系列。存在这个概念，可以认为是显像的显像。使用存在这个概念，其实只是为了方便说明。从这个角度思考，显像并没有掩盖本质，显像就是本质，也不是本质（从有限无限二元论来思考，等下讲）。破除这个二元论，并没有意味着就统一为一元论，实际上，应该称为有限/无限二元论。例如，看到杯子，我们常常认为，它就在那里，不是我的，这种想法表明杯子的实在性。也是表象论的想法。杯子的显现是一种不以人们好恶意志转移的，但这种显现为无限（就是无限视角观察）。所以杯子的存在，是一种以无限二元论思考。当杯子以有限视角观察，还原为单一个体，就变成一种充实直观的东西了。所以说有限的显现是在它自身有限性中表明，称为“有限显现”；当它要求超越有限性走向无限，显示出一个系列，从无限视角观察，不仅仅是一个sample，是“有限中的无限”。杯子有超越它自身存在的潜能，去显现整个系列，但也有它局限性：它只是现象的一个侧面，无限中的有限。本质是什么？从二元论角度看，本质和实体密切相关，从上面论证来看，存在既然是不存在的，只是一个个显现，本质本应该的用个别显露物去表露无限显露物。从这个角度思考，本质无法触及，第一本质根本不存在，第二，我们没有无限的samples去表露。再深入点，现象它自身是存在吗？我们认为显现只是表露它自身和整个系列，只能被它自身存在，从而我们能将显现和存在剥离，显现的本质就是不再和任何存在对立的“显现”。既然剥离了显现和存在（硬核的）关系，那么显现存在吗？（或说显现是硬核的吗？这个问题后面再说，我们先说存在和现象关系）&lt;/p&gt;

&lt;p&gt;我们认为，存在的现象，不足以揭示现象的存在。这里的“存在”和上面的存在一样，都是不存在的，只是为了方便说明和形象，特意造了一个实体。存在自身会散发出现象，因此有了一种“存在的现象”（这里我们先假设现象存在）。但这种存在的现象（显现），它真的能够揭露或它的本性真的与向我展现的存在物的存在一样吗？就是说存在的现象，真的能够揭示现象的存在吗？具体来讲，我们看到红色的花，真的能够认识红色吗？红色为存在的现象，不足以揭示现象（前面我们认为存在也是一种现象，从无限视角观察，这里的现象指红色的本体）的存在。所以说本质不在实体中（红色这个概念，不在红色的花里面）。现在我们讨论对象与存在关系，具体来讲，红色的花这个对象存在（存在红色）不？对象既不揭露存在（红色的花无法揭露红色），但也不掩盖存在（抛开红色的花去谈红色也是不行）。所以说本质不在对象中，而是对象的意义。那么，本质和存在是什么关系呢？如果我们认为，存在只是揭示本质的条件，但（现象的）存在却无法转发为存在的现象，而我们又想通过考察存在的现象对本质说点什么，这就是矛盾了。但实际上我们确实能够通过存在的现象对存在说点什么（海德格尔本体状-本体论的），为了统一，只要我们思考一下，不把存在看成揭示对象本质的条件，而是看成概念，就和我们前面讲的“存在的现象，不足以揭示现象的存在”吻合（同时也意味着，存在只是一个概念，从二元视角看，就解释了为什么一千个人眼中有一千个哈姆雷特。我想。）&lt;/p&gt;
</description>
        <pubDate>Sun, 26 May 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/huxblog-boilerplate/2019/05/26/philosophy/</link>
        <guid isPermaLink="true">http://localhost:4000/huxblog-boilerplate/2019/05/26/philosophy/</guid>
        
        <category>哲学</category>
        
        
      </item>
    
      <item>
        <title>高效的秘密</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on. ”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;高效的原则&quot;&gt;高效的原则&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;掌控力激发动力，掌控力与自信心之间的关系，你是如何思考的？高效前提是动力，就行发动机一样。&lt;/li&gt;
  &lt;li&gt;保持与专注。&lt;/li&gt;
  &lt;li&gt;构建合理、高效的心智模式，包括预判性规划、想象性能力。只要坚持思考，就成功了一半。我们每天的生活中，80%以上的情况是重复性程序性的工作，缺乏突破性思考和规划，这是人怕折腾，怕思考，又想进步，又不想思考、不想努力。&lt;/li&gt;
  &lt;li&gt;目标与延展目标，目标的分解与解构，目标重构和merge，达到目标的评价标准，你是如何设定评价标准和执行反馈手段的？我的一个信念是，each action should be recorded，每个动作都应该被记录，有了记录，然后才可能被分析，记录和分析，是反馈的手段。&lt;/li&gt;
  &lt;li&gt;逆反情景假设，多多思考“如何不是这样呢？”，跳出现有的思考模式和框架，正在追求的目标是否合理，总之，要始终保持思考，“你错了吗”。&lt;/li&gt;
  &lt;li&gt;寻找精益求精的敏捷思维。&lt;/li&gt;
  &lt;li&gt;内驱力。&lt;/li&gt;
  &lt;li&gt;把决策权交给能直接解决问题的人。多样化人才的选拔与评估。&lt;/li&gt;
  &lt;li&gt;学点数学，利用贝叶斯法则解决概率问题。&lt;/li&gt;
  &lt;li&gt;创新的本质。把熟悉的元素变得不同，具备极其创造力的人，很多都是知识的中介，他们懂得在不同行业、领域、群体中传递知识。排序和组合，是创新的一种手段。第二种创新性，我认为是无中生有，就是一开始忘记现有的表象，从根基，从感觉中出发，捕捉那一点的不同和灵光。创新，就那一点点。创新并不是一次就完成，会存在一次两次多次，也可以推倒重来，世界上没有什么是固定的。&lt;/li&gt;
  &lt;li&gt;数据的收集和分析。&lt;/li&gt;
  &lt;li&gt;高效的秘密，还来自团队合作的秘密，如如何激发合作，如何有效管理他人。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;决断力&quot;&gt;决断力&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;你是否觉察到自己的判断和决策中存在有“目光所及，便是一切”的“聚光效应”问题？&lt;/li&gt;
  &lt;li&gt;为什么决策需要流程？那是我们自身的缺点不足以规范我们思考和判断的过程。&lt;/li&gt;
  &lt;li&gt;利弊清单是比较原始的辅助决策的材料。&lt;/li&gt;
  &lt;li&gt;到底是什么阻止了你进行【有效】的决策？第一，思维的狭隘，意思是将选择局限在某个极小范围内，以二元化思维看待选择，A or B？第二，“证实倾向”，习惯性对某种情况迅速做出判断，潜意识中认为A 好过B，对某个东西，团队有好感。有时，我们认为某事件是真实的，而更加收集信息去支持这种判断，事后还庆幸自己做了一个理智的决策，这是非常糟糕的，自己屏蔽自己。证实性倾向决策，这种偏向性决策令我们快速适应做出决策，但却不一定是最优，甚至可能更糟糕。第三，短期的情绪。我们在决策时候，会混淆着情绪因素。&lt;/li&gt;
  &lt;li&gt;决策的流程中隐含着非优化因素：[a] 面临选择时，思维的狭隘限制了选择的范围。[b] 分析选项时，证实倾向使得你收集有利于自己的、当下认为“最好的选择”的信息。[c] 做出选择时，短期情绪令你常常做出错误判断。[d] 接受后果时，你对未来的走势估计太过自信。&lt;/li&gt;
  &lt;li&gt;针对上述问题[a]，如何避免思维的狭隘？1. 时刻提醒自己，不要专注与当前的选项。2. 考虑机会成本，假如我选择了这个，我会损失哪些？3. 考虑消失选项，假如这个、这些选项都没有了，你该如何做选择？4. 多目标追踪，尝试同时思考多个选择，但不是每个目标的权重都一样，是有区别的。警惕虚假选择：如何，核战，当前政策，或投降。5. 在选择后果中，积极在预防心态和促进心态转换。6. 争取两个都选，而不是“非此即彼”。我全都要，小孩子才做选择。7. 当选择太多了，感觉无法控制，那就去寻找已经帮你解决了问题的人。选项的来源，可以是来自事物的外部，也可以来自于事物的内部。将你精挑细选的选择编入决策的“问题列表”中。使用类比思维，来扩大选项。8.当你可以从丰富的选择中进行挑选时，你为什么非得自己提出自己的想法？懒惰不好吗？&lt;/li&gt;
  &lt;li&gt;针对上述问题[b]，如何避免证实性思维？把假设放在现实中去检验，实践是检验真理的唯一标准。a证实性思维让我们偏向考虑“好”的，那么，逆向思考，考虑相反情况如何？b多人合作决策，通过不同意见来抵消中和我们的证实偏向。c为了获得更多可靠的信息，我们可以反向问问题。d放大和缩小，缩小内视观点，放大外视观点，内视是在所处环境下，我们对齐评价。外视观点下，在这种情况下，事情是如何呈现的？通过特写，添加外视观点中缺少的内容。e尝试，可以小范围去验证我们的理论和猜想。&lt;/li&gt;
  &lt;li&gt;针对上述问题[c]，如何避免情绪掺杂？在做出决策之前，留出一段距离来思考。a如何克服短期情绪？常常有两种情绪左右我们决策，一是曝光效应：我们喜欢自己熟悉的东西。二是损失厌恶：损失使人痛苦，收益使人愉快，前者心理感受程度更为强烈些。b假设10分钟后，你对这个决定会有什么感觉？假如10个月后，你对这个决定有什么感觉？假如10年后，你对这个决定有什么感觉？通过拉长时间，克服短期紧张、害怕、恐惧，平衡情绪。c尊重你核心价值观。一个决策令人沮丧，可能是它常常与你的核心价值观发生了冲突。&lt;/li&gt;
  &lt;li&gt;针对上述问题[d]，做好出错的准备。a未来不是一点，我们针对错误的决策，评估它涉及的范围，对非常坏、到非常好的结果进行思考。b失败的原因是什么？对逆境和成功做出预期和准备。对于无法预见的结果，设置一个阈值，一个安全系数。c设置一个绊网，令我们清醒过来，重新思考这种作法，提供一个新认识，让我们有一个选择要做。&lt;/li&gt;
  &lt;li&gt;流程与决策。建立一套相对完备的信息收集、分析和决策流程，减少人为误差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;心智模式&quot;&gt;心智模式&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;（我想，引入图论与场论）在思考人的心时，有各种概念混杂，针对某特定场（模态，状态，场合），会有一组紧密联系的概念。但多个场中的概念会混杂。所以思考之所以混乱，是因为没有分清场，以及场和场之间的联系。例如“我想成功”，这个场，本身包含若干场。例如“如何做好一件事”，“如何发现局势、分析判断局势”，“我自身条件是什么”等等。“如何做好一件事”这个场又包含“如何发现问题，解决问题”等等，是当前问题的主要矛盾。“如何发现问题，解决问题”又是与“如何发现局势、分析判断局势”，“我自身条件是什么”相关，是它们各自的小矛盾（次要矛盾），但思考的某个阶段，又会变成主要矛盾。&lt;/li&gt;
  &lt;li&gt;所以场构成心智模式。&lt;/li&gt;
  &lt;li&gt;概念与联系构成场的基本元素，概念、联系与顺序构成一组动作，也就是原则，准则。意思是，“认为应该这样子去做”。所谓“行动的准则”，只不过的（概念+联系+顺序），而信念参与其中，影响联系的状态与哪些概念，什么顺序。&lt;/li&gt;
  &lt;li&gt;谈选择：价值观往往会导致选择，选择带有情感（痛苦+愉快）。选择掺杂感情假如重些，叫抉择。选择的对象有很多，例如现实，物质。&lt;/li&gt;
  &lt;li&gt;谈错误：错误不可避免。出现错误会影响人的情感。错误也是问题。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;谈反思：反思是调整一切心智模式的根源。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;谈成功：用以上小场，构成大场，成功 = 信心+现实（局势）+目标+发现问题解决问题。谈发现问题，能力包含洞察力，这个能力也是时局判断核心。解决问题：剖析问题，设计方案，落实步骤。若把成功看成是实践的典范，那么成功是理论与实践的主客体统一。事物的发展并不是一帆风顺的，错误在成功过程中，不可避免。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;人的情感是若干种资源的表现，我想可以用一种情感模型来表述，一种心智模式如下
&lt;img src=&quot;/img/mind.jpg&quot; alt=&quot;mind&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;情感和智能是两个不同东西，情感会影响智能，或许我们的机器并不需要情感。情感应该是一系列小资源的有机组合和表征，情感会抑制、或激活部分资源进而影响智能。那么人的价值观、目标、理想、信念以及三观等这些高级概念（超我）、乃至自我ego是在哪里？自我在于认识我是谁，是协调本我、超我之间的枢纽。我认为：本我是未经选择的资源表征，或者选择激活器对资源的抑制、激活不够。或者，所谓三我：本我、自我、超我，都是发生在“选择抽取与批评”与“选择激活”之间互动关系间。&lt;/li&gt;
  &lt;li&gt;Ps：我认为，人的智力是有限的，人能够认识这个世界，但认识不够完全，很多东西正由于人的智力存在缺陷导致我们反复、用其他概念去代替本质，“认识是螺旋上升的”，这种情况在人类面前，更加普遍和突出。人存在一种能够认识一切的能力，或许这种能力低级智能（如人类）存在，高级人类也存在，那么这种能力的下限在哪里？从神经元角度看，或者1400亿神经元超过人类（140亿），或许我们能够制造一种1400000…亿的智能体，该智能体也具有认识一切的能力。人们之所以不能提出一种意识模型，是因为人们受到意识颗粒度感知限制，例如我们可以感知快乐，却无法感知快乐的细微过程和快乐的构成。物质是如何有意识的？意识是否只有人类独有？是否万物都有意识？我认为，意识应该被认为的由多种小意识的有机构成，例如“念头”这个东西，并不纯正，存在很多同时，并行的念头。意识的基础是关系，那么，宇宙也有意识？人们使用很多概念去表征自己的意识和想法，其实这个描述并不精确。
也许通过表征学习，例如数据，我们并不需要去解构意识的本质和结构。我把这种方法称为模拟意识。&lt;/li&gt;
  &lt;li&gt;人类应该有个“内在感知器和抽样器”能够对意识海进行感知和抽取。所谓“内在性错觉”好像答案已经存在似的，是因为感知器还没对意识海进行抽取，实际上意识已经存在，但还没被感知。我认为，可能“洞察力”就是那个“内在感知器和抽样器”。所谓的“自我”，可能就是内建感知器和抽样器的一种工作方式。&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 22 May 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/huxblog-boilerplate/2019/05/22/mind/</link>
        <guid isPermaLink="true">http://localhost:4000/huxblog-boilerplate/2019/05/22/mind/</guid>
        
        <category>思维</category>
        
        
      </item>
    
  </channel>
</rss>
